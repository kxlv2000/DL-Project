{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torchtext==0.4.0","metadata":{"execution":{"iopub.status.busy":"2023-04-19T11:25:58.081198Z","iopub.execute_input":"2023-04-19T11:25:58.081725Z","iopub.status.idle":"2023-04-19T11:26:12.983737Z","shell.execute_reply.started":"2023-04-19T11:25:58.081676Z","shell.execute_reply":"2023-04-19T11:26:12.982535Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting torchtext==0.4.0\n  Downloading torchtext-0.4.0-py3-none-any.whl (53 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torchtext==0.4.0) (4.64.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from torchtext==0.4.0) (1.16.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from torchtext==0.4.0) (1.13.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchtext==0.4.0) (2.28.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchtext==0.4.0) (1.21.6)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.4.0) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.4.0) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.4.0) (1.26.14)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.4.0) (2.1.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->torchtext==0.4.0) (4.4.0)\nInstalling collected packages: torchtext\n  Attempting uninstall: torchtext\n    Found existing installation: torchtext 0.14.0\n    Uninstalling torchtext-0.14.0:\n      Successfully uninstalled torchtext-0.14.0\nSuccessfully installed torchtext-0.4.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\nimport random\nimport numpy as np\n\nSEED = 1234\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2023-04-19T11:26:15.403221Z","iopub.execute_input":"2023-04-19T11:26:15.403616Z","iopub.status.idle":"2023-04-19T11:26:15.412726Z","shell.execute_reply.started":"2023-04-19T11:26:15.403574Z","shell.execute_reply":"2023-04-19T11:26:15.411718Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizer\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2023-04-19T11:26:18.087979Z","iopub.execute_input":"2023-04-19T11:26:18.088345Z","iopub.status.idle":"2023-04-19T11:26:22.038236Z","shell.execute_reply.started":"2023-04-19T11:26:18.088312Z","shell.execute_reply":"2023-04-19T11:26:22.037277Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"338214644498420b8364f6f427864a6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c60ec9f6d67c4c73af1ce670a5eabd18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04d9a5b0918e4121b7cada2d05b097c7"}},"metadata":{}}]},{"cell_type":"code","source":"init_token_idx = tokenizer.cls_token_id\neos_token_idx = tokenizer.sep_token_id\npad_token_idx = tokenizer.pad_token_id\nunk_token_idx = tokenizer.unk_token_id\n\nprint(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T11:26:24.154278Z","iopub.execute_input":"2023-04-19T11:26:24.155415Z","iopub.status.idle":"2023-04-19T11:26:24.162475Z","shell.execute_reply.started":"2023-04-19T11:26:24.155364Z","shell.execute_reply":"2023-04-19T11:26:24.161402Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"101 102 0 100\n","output_type":"stream"}]},{"cell_type":"code","source":"max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n\nprint(max_input_length)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T11:26:26.248568Z","iopub.execute_input":"2023-04-19T11:26:26.248930Z","iopub.status.idle":"2023-04-19T11:26:26.254623Z","shell.execute_reply.started":"2023-04-19T11:26:26.248897Z","shell.execute_reply":"2023-04-19T11:26:26.253245Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"512\n","output_type":"stream"}]},{"cell_type":"code","source":"def tokenize_and_cut(sentence):\n    tokens = tokenizer.tokenize(sentence) \n    tokens = tokens[:max_input_length-2]\n    return tokens","metadata":{"execution":{"iopub.status.busy":"2023-04-19T11:26:28.583213Z","iopub.execute_input":"2023-04-19T11:26:28.583590Z","iopub.status.idle":"2023-04-19T11:26:28.588938Z","shell.execute_reply.started":"2023-04-19T11:26:28.583552Z","shell.execute_reply":"2023-04-19T11:26:28.587778Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from torchtext import data\n\nTEXT = data.Field(batch_first = True,\n                  use_vocab = False,\n                  tokenize = tokenize_and_cut,\n                  preprocessing = tokenizer.convert_tokens_to_ids,\n                  init_token = init_token_idx,\n                  eos_token = eos_token_idx,\n                  pad_token = pad_token_idx,\n                  unk_token = unk_token_idx)\n\nLABEL = data.LabelField(dtype = torch.float)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T11:26:31.856861Z","iopub.execute_input":"2023-04-19T11:26:31.857427Z","iopub.status.idle":"2023-04-19T11:26:31.874240Z","shell.execute_reply.started":"2023-04-19T11:26:31.857389Z","shell.execute_reply":"2023-04-19T11:26:31.873309Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from torchtext import datasets\n\ntrain_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n\ntrain_data, valid_data = train_data.split(random_state = random.seed(SEED))","metadata":{"execution":{"iopub.status.busy":"2023-04-19T11:26:34.516844Z","iopub.execute_input":"2023-04-19T11:26:34.517787Z","iopub.status.idle":"2023-04-19T11:32:30.510014Z","shell.execute_reply.started":"2023-04-19T11:26:34.517734Z","shell.execute_reply":"2023-04-19T11:32:30.508991Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"downloading aclImdb_v1.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:08<00:00, 10.4MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f\"Number of training examples: {len(train_data)}\")\nprint(f\"Number of validation examples: {len(valid_data)}\")\nprint(f\"Number of testing examples: {len(test_data)}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-19T11:32:35.675975Z","iopub.execute_input":"2023-04-19T11:32:35.676340Z","iopub.status.idle":"2023-04-19T11:32:35.682392Z","shell.execute_reply.started":"2023-04-19T11:32:35.676307Z","shell.execute_reply":"2023-04-19T11:32:35.681042Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Number of training examples: 17500\nNumber of validation examples: 7500\nNumber of testing examples: 25000\n","output_type":"stream"}]},{"cell_type":"code","source":"LABEL.build_vocab(train_data)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T11:32:38.543686Z","iopub.execute_input":"2023-04-19T11:32:38.544145Z","iopub.status.idle":"2023-04-19T11:32:38.605822Z","shell.execute_reply.started":"2023-04-19T11:32:38.544103Z","shell.execute_reply":"2023-04-19T11:32:38.604728Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 128\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ntrain_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n    (train_data, valid_data, test_data), \n    batch_size = BATCH_SIZE, \n    device = device)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T11:32:42.104496Z","iopub.execute_input":"2023-04-19T11:32:42.104887Z","iopub.status.idle":"2023-04-19T11:32:42.160714Z","shell.execute_reply.started":"2023-04-19T11:32:42.104853Z","shell.execute_reply":"2023-04-19T11:32:42.159482Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizer, BertModel\n\nbert = BertModel.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2023-04-19T11:32:44.399142Z","iopub.execute_input":"2023-04-19T11:32:44.399512Z","iopub.status.idle":"2023-04-19T11:33:02.450617Z","shell.execute_reply.started":"2023-04-19T11:32:44.399478Z","shell.execute_reply":"2023-04-19T11:33:02.449574Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6be48ad7416548f89e89307f19244087"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch.nn as nn\n\nclass BERTGRUSentiment(nn.Module):\n    def __init__(self,\n                 bert,\n                 hidden_dim,\n                 output_dim,\n                 n_layers,\n                 bidirectional,\n                 dropout):\n        \n        super().__init__()\n        \n        self.bert = bert\n        \n        embedding_dim = bert.config.to_dict()['hidden_size']\n        \n        self.rnn = nn.GRU(embedding_dim,\n                          hidden_dim,\n                          num_layers = n_layers,\n                          bidirectional = bidirectional,\n                          batch_first = True,\n                          dropout = 0 if n_layers < 2 else dropout)\n        \n        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, text):\n        \n        #text = [batch size, sent len]\n                \n        with torch.no_grad():\n            embedded = self.bert(text)[0]\n                \n        #embedded = [batch size, sent len, emb dim]\n        \n        _, hidden = self.rnn(embedded)\n        \n        #hidden = [n layers * n directions, batch size, emb dim]\n        \n        if self.rnn.bidirectional:\n            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n        else:\n            hidden = self.dropout(hidden[-1,:,:])\n                \n        #hidden = [batch size, hid dim]\n        \n        output = self.out(hidden)\n        \n        #output = [batch size, out dim]\n        \n        return output","metadata":{"execution":{"iopub.status.busy":"2023-04-19T11:33:07.618808Z","iopub.execute_input":"2023-04-19T11:33:07.619733Z","iopub.status.idle":"2023-04-19T11:33:07.632452Z","shell.execute_reply.started":"2023-04-19T11:33:07.619693Z","shell.execute_reply":"2023-04-19T11:33:07.631353Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"HIDDEN_DIM = 256\nOUTPUT_DIM = 1\nN_LAYERS = 2\nBIDIRECTIONAL = True\nDROPOUT = 0.25\n\nmodel = BERTGRUSentiment(bert,\n                         HIDDEN_DIM,\n                         OUTPUT_DIM,\n                         N_LAYERS,\n                         BIDIRECTIONAL,\n                         DROPOUT)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T11:33:10.710937Z","iopub.execute_input":"2023-04-19T11:33:10.711872Z","iopub.status.idle":"2023-04-19T11:33:10.741958Z","shell.execute_reply.started":"2023-04-19T11:33:10.711816Z","shell.execute_reply":"2023-04-19T11:33:10.740964Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')","metadata":{"execution":{"iopub.status.busy":"2023-04-19T11:33:13.886429Z","iopub.execute_input":"2023-04-19T11:33:13.887605Z","iopub.status.idle":"2023-04-19T11:33:13.896396Z","shell.execute_reply.started":"2023-04-19T11:33:13.887555Z","shell.execute_reply":"2023-04-19T11:33:13.894892Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"The model has 112,241,409 trainable parameters\n","output_type":"stream"}]},{"cell_type":"code","source":"for name, param in model.named_parameters():                \n    if name.startswith('bert'):\n        param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2023-04-19T11:33:16.311505Z","iopub.execute_input":"2023-04-19T11:33:16.312184Z","iopub.status.idle":"2023-04-19T11:33:16.318000Z","shell.execute_reply.started":"2023-04-19T11:33:16.312147Z","shell.execute_reply":"2023-04-19T11:33:16.316911Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchvision.transforms as transforms\n\nN_EPOCHS = 30\n\nbest_valid_loss = float('inf')\n\nfor epoch in range(N_EPOCHS):\n    \n    start_time = time.time()\n    \n    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n        \n    end_time = time.time()\n        \n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n        \n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'tut6-model.pt')\n    \n    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2023-04-19T12:56:16.997746Z","iopub.execute_input":"2023-04-19T12:56:16.998147Z","iopub.status.idle":"2023-04-19T16:14:52.495066Z","shell.execute_reply.started":"2023-04-19T12:56:16.998114Z","shell.execute_reply":"2023-04-19T16:14:52.493787Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Epoch: 01 | Epoch Time: 6m 36s\n\tTrain Loss: 0.684 | Train Acc: 51.71%\n\t Val. Loss: 0.373 |  Val. Acc: 86.28%\nEpoch: 02 | Epoch Time: 6m 36s\n\tTrain Loss: 0.675 | Train Acc: 52.70%\n\t Val. Loss: 0.343 |  Val. Acc: 87.09%\nEpoch: 03 | Epoch Time: 6m 36s\n\tTrain Loss: 0.671 | Train Acc: 52.20%\n\t Val. Loss: 0.324 |  Val. Acc: 87.42%\nEpoch: 04 | Epoch Time: 6m 36s\n\tTrain Loss: 0.671 | Train Acc: 52.73%\n\t Val. Loss: 0.418 |  Val. Acc: 81.69%\nEpoch: 05 | Epoch Time: 6m 36s\n\tTrain Loss: 0.679 | Train Acc: 52.40%\n\t Val. Loss: 0.338 |  Val. Acc: 87.83%\nEpoch: 06 | Epoch Time: 6m 36s\n\tTrain Loss: 0.662 | Train Acc: 53.91%\n\t Val. Loss: 0.307 |  Val. Acc: 88.28%\nEpoch: 07 | Epoch Time: 6m 36s\n\tTrain Loss: 0.675 | Train Acc: 52.47%\n\t Val. Loss: 0.309 |  Val. Acc: 88.43%\nEpoch: 08 | Epoch Time: 6m 36s\n\tTrain Loss: 0.654 | Train Acc: 54.01%\n\t Val. Loss: 0.301 |  Val. Acc: 88.45%\nEpoch: 09 | Epoch Time: 6m 36s\n\tTrain Loss: 0.667 | Train Acc: 53.03%\n\t Val. Loss: 0.321 |  Val. Acc: 87.28%\nEpoch: 10 | Epoch Time: 6m 36s\n\tTrain Loss: 0.655 | Train Acc: 53.64%\n\t Val. Loss: 0.310 |  Val. Acc: 87.74%\nEpoch: 11 | Epoch Time: 6m 36s\n\tTrain Loss: 0.667 | Train Acc: 54.17%\n\t Val. Loss: 0.336 |  Val. Acc: 86.34%\nEpoch: 12 | Epoch Time: 6m 36s\n\tTrain Loss: 0.668 | Train Acc: 52.76%\n\t Val. Loss: 0.280 |  Val. Acc: 89.31%\nEpoch: 13 | Epoch Time: 6m 36s\n\tTrain Loss: 0.665 | Train Acc: 53.69%\n\t Val. Loss: 0.278 |  Val. Acc: 89.23%\nEpoch: 14 | Epoch Time: 6m 36s\n\tTrain Loss: 0.665 | Train Acc: 52.53%\n\t Val. Loss: 0.290 |  Val. Acc: 88.88%\nEpoch: 15 | Epoch Time: 6m 36s\n\tTrain Loss: 0.659 | Train Acc: 54.82%\n\t Val. Loss: 0.370 |  Val. Acc: 85.29%\nEpoch: 16 | Epoch Time: 6m 36s\n\tTrain Loss: 0.664 | Train Acc: 53.61%\n\t Val. Loss: 0.284 |  Val. Acc: 88.64%\nEpoch: 17 | Epoch Time: 6m 36s\n\tTrain Loss: 0.652 | Train Acc: 54.09%\n\t Val. Loss: 0.295 |  Val. Acc: 87.76%\nEpoch: 18 | Epoch Time: 6m 36s\n\tTrain Loss: 0.652 | Train Acc: 54.70%\n\t Val. Loss: 0.393 |  Val. Acc: 82.75%\nEpoch: 19 | Epoch Time: 6m 36s\n\tTrain Loss: 0.655 | Train Acc: 55.03%\n\t Val. Loss: 0.279 |  Val. Acc: 88.97%\nEpoch: 20 | Epoch Time: 6m 36s\n\tTrain Loss: 0.653 | Train Acc: 54.84%\n\t Val. Loss: 0.279 |  Val. Acc: 88.71%\nEpoch: 21 | Epoch Time: 6m 36s\n\tTrain Loss: 0.664 | Train Acc: 53.69%\n\t Val. Loss: 0.276 |  Val. Acc: 89.74%\nEpoch: 22 | Epoch Time: 6m 36s\n\tTrain Loss: 0.646 | Train Acc: 54.22%\n\t Val. Loss: 0.271 |  Val. Acc: 88.70%\nEpoch: 23 | Epoch Time: 6m 36s\n\tTrain Loss: 0.651 | Train Acc: 55.18%\n\t Val. Loss: 0.252 |  Val. Acc: 90.41%\nEpoch: 24 | Epoch Time: 6m 36s\n\tTrain Loss: 0.661 | Train Acc: 53.86%\n\t Val. Loss: 0.265 |  Val. Acc: 88.91%\nEpoch: 25 | Epoch Time: 6m 36s\n\tTrain Loss: 0.633 | Train Acc: 55.30%\n\t Val. Loss: 0.253 |  Val. Acc: 90.26%\nEpoch: 26 | Epoch Time: 6m 36s\n\tTrain Loss: 0.650 | Train Acc: 54.60%\n\t Val. Loss: 0.320 |  Val. Acc: 86.55%\nEpoch: 27 | Epoch Time: 6m 36s\n\tTrain Loss: 0.646 | Train Acc: 53.17%\n\t Val. Loss: 0.254 |  Val. Acc: 90.31%\nEpoch: 28 | Epoch Time: 6m 36s\n\tTrain Loss: 0.652 | Train Acc: 54.14%\n\t Val. Loss: 0.247 |  Val. Acc: 90.54%\nEpoch: 29 | Epoch Time: 6m 36s\n\tTrain Loss: 0.652 | Train Acc: 55.51%\n\t Val. Loss: 0.253 |  Val. Acc: 90.28%\nEpoch: 30 | Epoch Time: 6m 36s\n\tTrain Loss: 0.641 | Train Acc: 55.25%\n\t Val. Loss: 0.249 |  Val. Acc: 90.16%\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch.optim as optim\nimport time\nimport numpy as np\n\n\noptimizer = optim.Adam(model.parameters())\ncriterion = nn.BCEWithLogitsLoss()\nmodel = model.to(device)\ncriterion = criterion.to(device)\n\ndef binary_accuracy(preds, y):\n    \"\"\"\n    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n    \"\"\"\n\n    #round predictions to the closest integer\n    rounded_preds = torch.round(torch.sigmoid(preds))\n    correct = (rounded_preds == y).float() #convert into float for division \n    acc = correct.sum() / len(correct)\n    return acc\n\n\ndef mixup_data(x, y, alpha=0.2):\n    lam = np.random.beta(alpha, alpha)\n    batch_size = x.size()[0]\n    index = torch.randperm(batch_size)\n    mixed_x = lam * x + (1 - lam) * x[index, :]\n    y_a, y_b = y, y[index]\n    return mixed_x, y_a, y_b, lam\n\ndef train(model, iterator, optimizer, criterion):\n    \n    epoch_loss = 0\n    epoch_acc = 0\n    alpha = 0.2\n    \n    model.train()\n    \n    for batch in iterator:\n        \n        optimizer.zero_grad()\n        \n        # Mixup the batch data\n        mixed_text, label_a, label_b, lam = mixup_data(batch.text, batch.label, alpha)\n        \n        mixed_text = mixed_text.long().to('cuda')\n\n        # Predict on the mixed data\n        predictions = model(mixed_text).squeeze(1)\n        \n        # Calculate the loss\n        loss = lam * criterion(predictions, label_a) + (1 - lam) * criterion(predictions, label_b)\n        \n        # Calculate the accuracy\n        acc = binary_accuracy(predictions, label_a)\n        \n        # Backpropagate the gradients\n        loss.backward()\n        \n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        \n    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n\ndef evaluate(model, iterator, criterion):\n    \n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.eval()\n    \n    with torch.no_grad():\n    \n        for batch in iterator:\n\n            predictions = model(batch.text).squeeze(1)\n            \n            loss = criterion(predictions, batch.label)\n            \n            acc = binary_accuracy(predictions, batch.label)\n\n            epoch_loss += loss.item()\n            epoch_acc += acc.item()\n        \n    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n\ndef epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs","metadata":{"execution":{"iopub.status.busy":"2023-04-19T11:46:53.094680Z","iopub.execute_input":"2023-04-19T11:46:53.095073Z","iopub.status.idle":"2023-04-19T11:46:53.491898Z","shell.execute_reply.started":"2023-04-19T11:46:53.095039Z","shell.execute_reply":"2023-04-19T11:46:53.490725Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(torch.load('tut6-model.pt'))\n\ntest_loss, test_acc = evaluate(model, test_iterator, criterion)\n\nprint(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2023-04-19T16:17:00.318032Z","iopub.execute_input":"2023-04-19T16:17:00.318810Z","iopub.status.idle":"2023-04-19T16:20:26.768552Z","shell.execute_reply.started":"2023-04-19T16:17:00.318771Z","shell.execute_reply":"2023-04-19T16:20:26.767373Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Test Loss: 0.230 | Test Acc: 91.04%\n","output_type":"stream"}]}]}